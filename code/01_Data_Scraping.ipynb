{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to scrape the data we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests, time, csv, json, re  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports the libraries we need to scrape the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Querying Reddit and saving raw data in .json format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function for creating a logfile and formatting file names with a unique timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_format_log(file_path, \n",
    "                        logfile = '../assets/file_log.txt', \n",
    "                        now = round(time.time()), \n",
    "                        file_description = None): \n",
    "   \n",
    "    try:\n",
    "        ext = re.search('(?<!^)(?<!\\.)\\.(?!\\.)', file_path).start() \n",
    "    except:\n",
    "        raise NameError('Please enter a relative path with a file extension.') \n",
    "    \n",
    "    stamp = re.search('(?<!^)(?<!\\.)[a-z]+_[a-z]+(?=\\.)', file_path).start()\n",
    "    formatted_name = f'{file_path[:stamp]}{now}_{file_path[stamp:]}'  \n",
    "    if not file_description:\n",
    "        file_description = f'Pull: {time.asctime(time.gmtime(now))}'\n",
    "    with open(logfile, 'a+') as f:\n",
    "        f.write(f'{formatted_name}: {file_description}\\n')\n",
    "    return formatted_name, now, file_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function for collecting comments and parsing into a dataframe with the features of interest, saving out the raw data for each pull. Request loop inspired: [(Source)](https://www.reddit.com/r/pushshift/comments/89pxra/pushshift_api_with_large_amounts_of_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_query(url, n_samples, before=None, after=None):\n",
    "    last_comment = round(before)\n",
    "    comment_list = []\n",
    "    \n",
    "    run = 1\n",
    "    while len(comment_list) < n_samples:\n",
    "        \n",
    "        try:\n",
    "            print(f'Starting query {run}')\n",
    "            \n",
    "            params = {'subreddit':'competitivehs',\n",
    "              'sort':'desc',\n",
    "              'size':1000,\n",
    "              'before':last_comment-1,\n",
    "              'after':after,\n",
    "             }\n",
    "                \n",
    "            response = requests.get(url, params = params)\n",
    "            posts = response.json()['data']\n",
    "            \n",
    "            if len(posts) == 0:\n",
    "                last_comment = last_comment\n",
    "            else:\n",
    "                last_comment = posts[-1]['created_utc']\n",
    "                comment_list.extend(posts)\n",
    "                timestamp = posts[-1]['created_utc']\n",
    "                time.sleep(1) \n",
    "                run += 1\n",
    "        except:\n",
    "            if response.status_code != 200:\n",
    "                return f'Check status. Error code: {response.status_code}'\n",
    "            else:\n",
    "                return 'Error. Pull not completed.'\n",
    "    \n",
    "    formatted_name, now, file_description = filename_format_log(file_path =f'../data/raw_comments.json', now=timestamp)\n",
    "    with open(formatted_name, 'w+') as f:\n",
    "        json.dump(comment_list, f)\n",
    "    \n",
    "    print(f'Saved and completed query and returned {len(comment_list)} comments.')\n",
    "    print(f'Reddit text is ready for processing.')\n",
    "    return print(f'Last timestamp was {timestamp}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to query the pushshift.io api for 1000 posts at a time from the competitivehs subreddit and add those posts to a json file. Once all of the posts have been acquired, it saves the filename as the earliest timestamp that was queried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting query 1\n",
      "Starting query 2\n",
      "Starting query 3\n",
      "Starting query 4\n",
      "Starting query 5\n",
      "Starting query 6\n",
      "Starting query 7\n",
      "Starting query 8\n",
      "Starting query 9\n",
      "Starting query 10\n",
      "Saved and completed query and returned 10000 comments.\n",
      "Reddit text is ready for processing.\n",
      "Last timestamp was 1505757232.\n"
     ]
    }
   ],
   "source": [
    "KOFT_comments = reddit_query(url = 'https://api.pushshift.io/reddit/search/comment/',n_samples = 10000, before =1507593600 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This queries pushshift.io for 10,000 reddit comments from competitivehs during the KOFT time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/1505757232_raw_comments.json', 'r') as f:\n",
    "    KOFT_comments = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the comments from the KOFT json into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_parse(sample):\n",
    "    \n",
    "    col_list =  [\n",
    "                'author',\n",
    "                'body',\n",
    "                'created_utc',\n",
    "                ]\n",
    "    \n",
    "    comments_df = pd.DataFrame(sample)\n",
    "    comments_df = comments_df[col_list]\n",
    "    \n",
    "    \n",
    "    col_order =  [\n",
    "                'author',\n",
    "                'body',\n",
    "                'created_utc',\n",
    "                ]\n",
    "\n",
    "    return comments_df[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to parse the json files we load into memory and return a DataFrame containing the comments' author, body, and time created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOFT_comments_df =  reddit_parse(KOFT_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns the KOFT json into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "KOFT_comments_cleaned = KOFT_comments_df[~KOFT_comments_df.author.str.contains(\"\\[deleted\\]\")]\n",
    "KOFT_comments_cleaned = KOFT_comments_cleaned[~KOFT_comments_df.body.str.contains(\"\\[deleted\\]\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets rid of all deleted posts from the KOFT dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "KOFT_comments_cleaned = KOFT_comments_cleaned[~KOFT_comments_df.author.str.contains(\"bot\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets rid of all posts by bots from the KOFT dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOFT_comments_cleaned['KOFT'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifies all of the KOFT comments as being from KOFT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting query 1\n",
      "Starting query 2\n",
      "Starting query 3\n",
      "Starting query 4\n",
      "Starting query 5\n",
      "Starting query 6\n",
      "Starting query 7\n",
      "Starting query 8\n",
      "Starting query 9\n",
      "Starting query 10\n",
      "Saved and completed query and returned 10000 comments.\n",
      "Reddit text is ready for processing.\n",
      "Last timestamp was 1485281935.\n"
     ]
    }
   ],
   "source": [
    "MSOG_comments = reddit_query(url = 'https://api.pushshift.io/reddit/search/comment/',n_samples = 10000, before = 1488283200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This queries pushshift.io for 10,000 reddit comments from competitivehs during the MSOG time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/1485281935_raw_comments.json', 'r') as f:\n",
    "    MSOG_comments = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the MSOG json into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSOG_comments_df =  reddit_parse(MSOG_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns the MSOG json into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "MSOG_comments_cleaned = MSOG_comments_df[~MSOG_comments_df.author.str.contains(\"\\[deleted\\]\")]\n",
    "MSOG_comments_cleaned = MSOG_comments_cleaned[~MSOG_comments_df.body.str.contains(\"\\[deleted\\]\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets rid of all deleted posts from the MSOG dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "MSOG_comments_cleaned = MSOG_comments_cleaned[~MSOG_comments_df.author.str.contains(\"bot\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets rid of all posts by bots from the MSOG dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSOG_comments_cleaned['KOFT'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets all of the MSOG comments as not being from KOFT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_df = pd.concat([KOFT_comments_cleaned,MSOG_comments_cleaned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combines the KOFT and MSOG dataframes into a new dataframe called Complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_df['body'] = Complete_df.body.map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This removes all of the links that may still be present in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_df.drop_duplicates(inplace=True)\n",
    "Complete_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This drops all duplicates and resets the index so that we can convert our dataframe to json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_df.to_json(path_or_buf = '../data/Complete_dataframe.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exports the combined dataframe to json format to be used for EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
